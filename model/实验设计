如果新的mobilenetv2准确率可以达到71.4左右，那就可以开始展开--最初：res=4的v2：67.799995emmm就这样展开吧，第一层也不会主动变stride为2.变量恒等就好。
mini_imagenet--80epochs---63.7917   mini240:75.591(重做，expandrate这里是4，应该是6:77.566665649,77.76666259765625,77.583335,77.14166259(输出是640而不是1280时)，77.3583297(640--6_0.57)77.975(简化classifier,这种方式的可扩展性确实更好了)，78.95(简单地进行classifier rep)，79.175（对前者使用了dropout增加多样性）78.116(简化后比例变成4-0.81)，77.8917（简化后变成2,1.14）,76.4583(简化后边1,1.6)) 2-0.78:75.4249954223  triple:78.85
在尽量不改变原模型参数以及各个阶段模块的大小前提下，
对于rep的插入点，参照repvit，可以有
    1.最初的stem模块--对cifar100我们选择kernel_size为 7 imagenet我们选择kernelsize为13。这么做的意义就是最初就提取更加细致的局部特征，相较于传统vit的kernel_size=stride，hybrid模型搭建过程中可知overlap地提取可能更有利于最初的stem。
        2024-4-01发现uni主干没有加bn层。先对表现最好的118和1131再做一下实验。
        1.cifar部分可以将原本的3*3（3->32变为7*7的rep（channel：3->5，stride为2）+1*1的卷积进行通道变换(5->32)）这里的参数是864/895。基本相同(如果乘以mul_width对应相减)。
            *1.使用的rep组合为7-uni标准---62.879/62.639/65.739/66.379/66.36999      mini80:65.82499694824219/66.8249969(5)   mini240:76.8833(这里的结果其实是和下面78.433334区别在于额外的1*1卷积是否有bn层。而rep池不加1*1时 77.949),
                1没有rep后的1x1为77.52
                2.结合律 后面加 cifar:66.139  mini240:77.525--没啥影响...
                3.结合律 前后加
            2.添加额外的rep池，尤其是1*1卷积。----62.269/62.809/65.760/65.589/65.86999   mini:66.224998   mini240:78.433334  mini240:77.67499(true)
            3.浅试一下kernel_size为5的 ----62.159/62.18/66.16998/65.93/64.79999   mini:66.6083297(5)  mini240:78.058  seed:77.183
                1.
            4. kernel_size 为5+1*1  ---62.479/62.5/65.70/65.6599/65.93000  mini80:66.3833312/67.7249(5)    mini240:77.866
            ---以上效果很差，感觉是norm的缘故，将norm去除重新跑一次实验。(每组第二次结果)
            self.__setattr__('dil_bn_k{}_{}'.format(k, r), nn.BatchNorm2d(outchannels) if inchannels==outchannels and stride ==1 else nn.Identity())
            感觉这条代码设计很不合理，将后面的条件语句删除。重新跑一次，同时删除stem中间层，就直接是3-3(每组第三次结果)
            还可以后面加一个激活函数(每组第4次结果)
            **rep尝试每个分支后面加一个1*1的分支升维卷积(每组第5次结果)
            
            -5. 不使用uni和rep模块，就简单地组合7*7,5*5,3*3  67.699（这个是每个分支先过了relu的，但不可以。修改后的精度为：68.3）66.0  mini:65.691665   mini240:77.375
                1.9x9  mini240:77.299995
                2.11x11  mini 240:77.5
                    1. 额外加1x1 240:78.0083--貌似分支多了以后1x1就比较关键了
                3.13x13  mini 240:78.0250015
                    1. 额外加1x1 mini240: 78.09  seed:77.68
                4. 7x7并1x1 mini240：76.8083343
            6. 不使用uni和rep模块，就简单地组合5*5,3*3,   68.519/66.389  mini:66.25  mini240:77.25
            7. uni 9  mini240:77.47499847  cifar :66.0699
            *8. uni 11  mini240: 78.12 seed：77.48332 nobn：77.6916 （主干加bn：77.458）  cifar:65.73999
                1. 自选uni，池无1x1 76.91666  cifar:65.8600
                2. 自选uni，池有1x1 78.09166  cifar :66.080001
                3. 把uni后面提取各个感受野的1x1去掉 77.86666  cifar:65.23999
                4. 原uni加weight  minibn：77.0
                5. 4的weight初始化不为1，而是1/len  minibn：77.183
                6. 118去掉后面的1x1  minibn：77.10832
                7. 118的1x1换成可rep的  mini240: 77.1667
            9: 13的uni mini240:77.625   cifar :66.01999
            10: 自选uni 9 mini240:76.94166   cifar:65.38
            11: 自选uni + 1x1 9 mini240：77.116  cifar:66.25  
            12：自选uni + 追1x1 9 后追1*1：77.2916--添加weights后77.70833  cifar:65.8199
                1.新的一种weights动态构成方式，参照senet。  77.96666   cifar:65.7699
        2.亦或是在3*3之前添加一个7*7的rep模块，channel不变，stride为1. 
            1-4的设置追随1-1-1至1-1-4  
            1 --68.259/68.019  mini:66.275001
                1.后接1x1  mini 240 ：77.808334
            -2  --68.409/68.62  mini:66.533   mini240:77.891662
                1.加后接的1x1 cifar：68.6299 mini:240 :78.01666259 seed:77.47
            3 --68.089/67.97  mini:66.43333
            4 --67.970/67.88  mini:66.52500
            ---将norm去除再跑一次
        -3.对原本的3*3卷积进行rep操作，其实就是多加了一个1*1并行。---68.470   ---添加norm后再跑一次。68.10  mini:66.3916625   mini240:77.958335
        4.将2的模块放在后面，其实这里就是可以解释为一种类metaformer的结构。
    2.下采样模块
        1.正常rep_dw替代3*3dw  67.769  mini:65.758331
            1rep后不加bn层。67.569  mini:66.34166
        2.低维+一个3*3dw来提取抽象的特征，1*1后补升维。act前相加  67.860/68.09  mini:66.50833
        3.act后相加  66.830/68.279  mini:66.94999   mini240:76.95833587
        4.+的3*3为repdw---这里是act后相加  67.339/67.83  mini:66.491668
        5.pw换3*3 71.650   mini:68.5
        -6.pw换3*3rep  72.33 参数增长太大，换成1*1升维前先用一个3*3的dw进行token mixer 70.15/68.7400两个结果，也不知道哪个是真的，再跑一次吧**--结果是69.66999816894531。再验证一次，貌似中间层没stride 67.199  mini:64.125  mini240:76.375  seed:76.67
        --212,213,214,216这里没有区分stride2/1，因而修改的不仅仅是下采样模块。--也可以正好验证一下组合的效果。(这个失误是说最后小的分辨率不如一直大的跑吗？可以修改一下最后几个块从传统的32变成16--停留在14试试)
    3.分类器部分
        结尾部分的1*1卷积添加一些小分组的rep。这里的话讲道理是可以按group来玩rep--这个赛道好像是最后的了，按group来进行rep 67.49--原来rep双重act了，修改后的准确率为：  mini:67.28333282   mini240:77.48332
        1.利用可rep的1x1来变维度试一下。
        2. classifier的input通道从1280变成128  mini:76.949  cifar:68.68000
    4.主体部分-反向线性残差结构--411 413 415 417重跑，中间没stride(后来想了一下这里是没有错的，因为原本就已经条件语句判断stride为1了，没有歧义。)
        1. 全加   68.04/68.229 mini:65.391662     mini240:77.3333358
        -2. 上  68.51  mini:67.116668   mini240:77.8333358
            1. pw前添加dw层提高局部性，同时对dw进行rep 77.35832   cifar:66.88999
            2. 同时对dw进行rep 77.4000015  cifar:  67.66999
        3. 中  68.52/68.259   mini240:77.19999
            1.3x3和1x1后加1x1到original_channel,relu，expand_channel的ffn：78.28333代表这里需要进一步channel_mix  cifar:67.40000
            2.seblock,77.75   cifar: 68.589
            3.相较于1没有id_out就是没有类似residual的那条路。 77.32499694824219    cifar:67.009994
        4. 下  67.889  mini:66.15833282470703  mini240:77.708335
        5. 上中  68.32/67.959  mini:65.9416   mini240:77.491668   
        6. 上下  67.29  mini:65.89166259
        -7. 中下  67.45/68.580  mini:66.33333  mini240:76.95833
以上部分仅仅测试3*3,1*1，bnidentity的经典式rep即可。在各分组部分结果的基础上再进行组合。*实验完成后，对1-1-1到1-2-4中最好的结果进行norm的消融实验。
    5. 全加  72.41999816894531---重做，中间层没下采样。实际：68.18000030517578      mini:66.06666  mini240:78.01666  77.58  77.24  77.27   2-0.75:76.32  +se:76.616
        1 分支接可rep1x1 cifar:68.25  mini240:77.52
        2. cifar:67.0299  mini240:73.7916
        3. 不晓得
        4. 简化classifier 78.366668
    6. 维度冗余：
        1.group 遍历expand_rate，使用rep的方式，增强反向线性残差结构高维度的维度独立特性。
            1.全加  76.5666656494  76.90000  cifar:67.019996
            2.只针对下采样。 77.38333129882812   cifar:68.019996  
                1. group 遍历expand_rate，的部分和权重为1 主干权重也为1 77.3166656   cifar:68.139999
        2.减少expand_rate, 按比例扩增给width。 mini:240 77.43333435058594  77.43333  cifar:67.659996
    7. 小模型是不是对结尾的分辨率，更大更友好？--是模型都是这样，只不过更大的分辨率代表更多的显存和更长的时延。
        6,0.5  -6,1
        1.将传统的32倍缩为16倍。 cifar:68.65 mini240:78.091
            1.mini240:81.9249954   cifar:71.669
        2.将传统的32倍缩为8倍。  cifar:71.5099 mini240:79.5499
            1.mini240:82.93333    cifar:74.13
    8. repvit文章第四页右上角的段落描述：修改膨胀率和width之后，原本性能下降的block设计会比直接源模型调节超参性能更好。这是不是说当膨胀率和width不同时，同样的结构会展现出不同的性能
    9. classifier虽然只是1x1卷积，即线性层，但是随着分类类别的上升，大小会急剧上升。那能不能有一个中间层来先降低维度，如1000个类别，首先有100个二级类别，然后再从这100个二级类别中进一步辨别类别归属。 
        cifar: 62.97999954  mini240:70.175003051
        优点：时延特别小，
        改进：比如cifar-100，原本就有二级类别，如果细化损失，能否有更好的性能。
        out2*out1  cifar：2.32999
    10 分支后可rep的1x1可以理解为一种通道特征融合，特征泛化（充当weight的作用）
    11. 将mobilenet的反向线性残差结构修改为metaformer结构   76.866
        1.去掉下采样层后面的1x1
        2. 511改
mobile_vit_tiny_init_res4 69.26999664和 mobile_vit_tiny_best_res4 69.86000061的对比可知，合适的rep嵌入，有利于模型性能提升。
    12. mculike（flops不变，参数量增长百分之十）: 减少peek memory，一种是直接减少expand_rate，一个是这种 77.08333
        类511-5113  76.6583（模型大小变为了1.1m）  cifar:67.22  mini262:73.558(模型大小不变)
        类final10  77.3583
final:
1.正常  76.042 cifar : 66.4000
2.stem换大核uni  75.416  6 0.5 77.108 cifar:65.68999
3.stem去bn  74.7333  6 0.5 76.45833587  cifar:61.95999
4.下采样有bn  75.0666  6 0.5--75.858 cifar: 62.069
5.主干dw去skip  75.441  6 0.5 77.5250  cifar :66.2099
6.比例换6，0.5  77.9166  cifar: 65.6399  1280:77.25
7.增加group分支  6 0.5 77.0  cifar :66.30
8. 分支后加1x1  cifar :66.3099  mini:77.7416
9. 去除classifier的group分支 77.0499  1280:77.183
前面的conv2都是640，要改成1280重新做一遍
10. classifier不改变 77.93333  1280:77.27

11. stem 换1221  77.0250  1280:76.85832
12. 不加stem  77.891  1280:77.71666
13. 下采样不加rep 77.299 1280:77.7416
14. dw不要identity 77.5250  1280:77.17499  change:54.45000
15. rep 加dropout  0.25:77.0749  0.5:76.5750
16. dw加senet  77.375
17. 把rep换成repvit中的那种only结尾bn的范式。 74.9000
18. mculike  cifar:64.7099  mini:77.35

19. 感觉rep的训练时参数增长不明显，所以在9的基础上dw进行double dense。  78.10
20. 19基础上可以引入dropblock来增强分支的多样性。 77.5166  76.84,76.1667(优化了之后还是不行--这种方式引入多样性不行？)
21. 简化classifier，扩大width  78.308
效果不好就要看rep哪里的添加是对这个结构有用的，拆除反向线性和stem
22 只rep stem79.43333--基础上没有后边的1x1---79.06 pre1x1:78.59 pos: 79.0833  +1x1 换11：78.87 换 5：79.25 换9：79.1333
23 只rep 反向线性的上中  78.516
    1. 只rep pw 78.958
    2. 只rep dw 78.658  +weight(155): 79.158
24 22的基础上加dw的rep 78.68333 +weight:78.425（全1） 前者基础上div3:78.350 div2:78.0499  77.941665(全0.1) 79.2999（dense 1 其他0.5）  no identity:78.4749 no1x1:78.4333  no1x1_double dense: 78.87  double dense:78.40 only double dense: 79.716 +weight: or+div2:79.3166 or+div3:79.0667 79.325 triple dense:79.408 forth dens: 78.53 fifth: 79.3
25 22的基础上加pw的rep  78.98 +weight:78.88 double_dense: 79.5416 +weight:79.32499 or+bn 79.34 usebn:79.0499 no1x1:79.283 no_identity:79.22
22的基础上，dw和pw都有only_double:79.4833 +weight: 79.1666 or div:79.33 分支不同初始化:79.3833 +dropblock 5:78.56 +unequal:79.36 3: 79.10 +unequal:77.87

8.换meta

26. 想要使用rep来代替skip最关键的是原来的连接会将部分主干的正负进行翻转。如果使用了rep之后，因为relu的特性，主干负转正是没有影响的，但
由于relu将输入中的负数全部转为0，所以无法正常主干正转负。
    relu: 79.1416
    1.激活函数换leakrelu（考虑到负数）:79.033(0.01),79.45(0.1),selu:74.699(负方向类似relu6考虑)，elu:77.0916671, relu6:79.074996   gelu:79.383 
    norep:79.0250  onlylinear:79.0833  onlypw:78.6917  onlydw:79.5166  nodw:78.741   nopw:79.283   nolinear:79.6500   +stem:79.22499 nolinear+stem:79.65  nolinear+stem+doubledense:79.283  nolinear+stem+doubledense/div:79.75 nolinear+doubledense:79.075 nolinear+doubledense/div:79.2750 
dw 换do conv：原77.67   简化后double classifier3：78.75
no_linear_stem_doubledense_div_new_2: linear 多加分析层：78.5  no_linear_stem_doubledense_div_new:74.525
no_linear_stem_doubledense_div_06:77.9750
no_linear_stem_doubledense_div_pw: 79.350
no_linear_stem_doubledense_div_identobn:79.1167
no_linear_stem_doubledense_div_dw:79
no_linear_stem_both_doubledense_div_pwnoact:79.125
+no_linear_stem_doubledense_div_25:79.3917
no_linear_stem_both_doubledense_div:79.0833
no_linear_stem_doubledense_div_dwnoact: 79.1083\
no_linear_stem_doubledense_div_dwnoact_linearact:77.7250
+no_linear_doubledense:79.0750
+no_linear_doubledense_div: 79.2750
验证classifier rep不是蒸馏：和kd蒸馏相结合，在clsss和class3进行对比实验，采用不同的比率(r0.1 a0.9 77.333/77.5583 or r0.5 a0.5 78.2/78.6917)都证明双rep有涨点的。从实现上来看，本方法没有软标签，也没有像自蒸馏方法一样在多个层之间进行kl距离的对比和拟合，而是采用集成和rep的方法对以获取的特征进行over parameterizing类的分析。
doconvdiv: 全换之后有略微的涨点，但原论文说不推荐1x1使用，这里使用了反而有涨点的表现。
no_linear_stem:79.6500
no_linear_stem_doubledense:79.2833
no_linear_stem_doubledense_div:79.75


mcunet: inil:77.3583  impl 77.9917   0.8M 78.5167
inil_doubleclass: 78.6833 +bn 78.7750 +rep有下降(这里采用的是25式的rep，明显23式的兼容性更好一点)      inil_doubleclass_bn_08:79.450
impl 08_class_nobndrop: 79.5583
impl_08_class:79.4917
impl_08_class_stem:79.6416
impl_08_stem_dw:80.1833
impl_08_stem_pw:79.4583
impl_08_stem_pwdw:78.8917
impl_08_stem_dw_div3:79.9083
impl_08_stem_dw_nodiv:79.5083

repvgg测试div： inil: 79.3250 div2:79.1167  div3:80.1

后面要测试的就是系统性的加可rep1x1以及dov

repvit 80  初始 76.5667. class之后77.3667


